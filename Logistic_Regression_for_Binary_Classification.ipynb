{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression for Binary Classification problem"
      ],
      "metadata": {
        "id": "jGY2EdSl6KkE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overview**\n",
        "- This project implements a Logistic Regression algorithm from scratch to classify a dataset with two classes.\n",
        "- The algorithm uses gradient descent to learn optimal weights and then evaluates its performance using accuracy and a classification report."
      ],
      "metadata": {
        "id": "tmRLMiu1_lhL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3wh8CeM6J-K"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Synthetic Data"
      ],
      "metadata": {
        "id": "E2NoV1LV70xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_binary_synthetic_data(nFeatures=4, nSamples=40, test_ratio=0.2):\n",
        "    K = 2 # number of classes\n",
        "    # Generate random feature values between 0 and 1\n",
        "    X = np.round(np.random.rand(nSamples, nFeatures),3)\n",
        "    # print(X)\n",
        "\n",
        "    # Add some noise to X\n",
        "    N = nSamples\n",
        "    mid_N = int(N/2)\n",
        "    # The first half of the samples are scaled by 2, and the second half by 5, to introduce a distinction between two classes\n",
        "    X[:mid_N, :] = X[:mid_N, :] * 2\n",
        "    X[mid_N:N, :] = X[mid_N:N, :] * 5\n",
        "\n",
        "    test_set_size = int(nSamples * test_ratio / 2)\n",
        "    X_test_class1 = X[:test_set_size, :]\n",
        "    X_test_class2 = X[mid_N : mid_N+test_set_size, :]\n",
        "\n",
        "    # np.delete(array, obj, axis) function removes elements from an array along a specified axis.\n",
        "    X = np.delete(X, np.s_[0:test_set_size], 0)\n",
        "    # Remove from mid_N - test_set_size as after removing first 4 (test_set_size) rows, the index will be shifted\n",
        "    X_train = np.delete(X, np.s_[mid_N -test_set_size : mid_N], 0)\n",
        "\n",
        "    X_test = np.concatenate([X_test_class1, X_test_class2])\n",
        "\n",
        "    N_train = X_train.shape[0]\n",
        "    N_test = X_test.shape[0]\n",
        "\n",
        "    # Output variable is R\n",
        "    R_train = np.repeat([1,0], N_train/K, axis=0)\n",
        "    R_test = np.repeat([1,0], N_test/K, axis = 0)\n",
        "\n",
        "    return X_train, R_train, X_test, R_test"
      ],
      "metadata": {
        "id": "Gr7Jw-Zf6RYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train, X_test, y_test = generate_binary_synthetic_data(nFeatures=4, nSamples=40, test_ratio=0.2)\n",
        "# Add y_test as last column to X_test and print the Test Set with its labels\n",
        "print(np.c_[X_test,y_test])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OYc-4ku6vKC",
        "outputId": "a5bcdf6c-b283-487c-acd4-540def777e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.75  1.902 1.464 1.198 1.   ]\n",
            " [0.312 0.312 0.116 1.732 1.   ]\n",
            " [1.202 1.416 0.042 1.94  1.   ]\n",
            " [1.664 0.424 0.364 0.366 1.   ]\n",
            " [4.315 3.115 1.655 0.32  0.   ]\n",
            " [1.555 1.625 3.65  3.19  0.   ]\n",
            " [4.435 2.36  0.6   3.565 0.   ]\n",
            " [3.805 2.805 3.855 2.47  0.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Algorithm\n",
        "\n",
        "Implement logistic regression using Gradient Descent"
      ],
      "metadata": {
        "id": "-7Y-k5wj8YZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_regression(x,r,d,step_size=0.1,iterations=10):\n",
        "    x0 = np.repeat(1, len(x))\n",
        "    # A column of ones is added to x to account for the bias term, w_0\n",
        "    new_x = np.c_[x0, x]\n",
        "    # print(new_x)\n",
        "    w = []\n",
        "    d = 4\n",
        "    for j in range(0,d+1):\n",
        "        # Weights are randomly initialized close to zero.\n",
        "        w.append(np.random.uniform(-0.01, 0.01))\n",
        "    # print(w)\n",
        "\n",
        "    for test in range(0, iterations):\n",
        "        deriv_w = []\n",
        "        for j in range(0, d+1):\n",
        "            deriv_w.append(0)\n",
        "        for i in range(0, len(new_x)):\n",
        "            o = 0\n",
        "            for j in range(0, d+1):\n",
        "                o += w[j]*new_x[i][j]\n",
        "            # Compute the logistic function\n",
        "            y = 1/(1+math.exp(-o))\n",
        "            for j in range(0, d+1):\n",
        "                # Calculate the gradient (or derivative) of the loss function\n",
        "                deriv_w[j] += (r[i]-y) * new_x[i][j]\n",
        "        for j in range(d+1):\n",
        "            # Update the weights\n",
        "            w[j] += step_size * deriv_w[j]\n",
        "    # Return the final learned weights after training.\n",
        "    return(np.round(w,3))"
      ],
      "metadata": {
        "id": "0yKNcg_V66hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Logistic Regression model"
      ],
      "metadata": {
        "id": "155CPwpE9Wlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model is trained on the generated dataset with 4 features, 5000 iterations and learning rate (step_size) = 0.01.\n",
        "n_features = 4\n",
        "X_train, y_train, X_test, y_test = generate_binary_synthetic_data(nFeatures=n_features, nSamples=40, test_ratio=0.2)\n",
        "w_array = log_regression(X_train, y_train, n_features, step_size=0.01,iterations=5000)\n",
        "print(w_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmA9jRb_6_y6",
        "outputId": "da371327-7dc0-4a10-c19f-9ea3019f8c97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.996 -4.524 -3.352  1.558 -3.278]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Predictions\n",
        "- Above logistic regression model learns a set of weights (w_array) that define a decision boundary. These weights can be used to predict class labels for new samples"
      ],
      "metadata": {
        "id": "1nxzHRo99qvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w):\n",
        "    x0 = np.ones((X.shape[0], 1))  # Add bias term\n",
        "    X_new = np.c_[x0, X]\n",
        "\n",
        "    # Compute probability using sigmoid function\n",
        "    probs = 1 / (1 + np.exp(-np.dot(X_new, w)))\n",
        "\n",
        "    # Convert probabilities to binary labels (threshold at 0.5)\n",
        "    predictions = (probs >= 0.5).astype(int)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "PSYNKxn3-hbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get predictions on test set\n",
        "y_pred = predict(X_test, w_array)\n",
        "print(np.c_[y_test, y_pred])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnMQpMBH-jac",
        "outputId": "ad9c2ef0-d115-45c0-e089-29f40a286342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hesP5zcy7IUe",
        "outputId": "9f483183-61b2-47b1-e471-02667bfeb792"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model achieved 100% accuracy on the test set, which means all test samples were correctly classified"
      ],
      "metadata": {
        "id": "LVts-3gu-8Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0d26RY--RbD",
        "outputId": "0a3c2580-bff2-4139-f833-6bf88504295a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         4\n",
            "           1       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00         8\n",
            "   macro avg       1.00      1.00      1.00         8\n",
            "weighted avg       1.00      1.00      1.00         8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The model perfectly classified all test samples.\n",
        "- Logistic Regression algorithm has effectively learned the decision boundary between the two classes"
      ],
      "metadata": {
        "id": "1iJkDscH_AEr"
      }
    }
  ]
}